{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import numpy as np \n",
    "faces=np.load(\"mnist_train_images.npy\") \n",
    "labels=np.load(\"mnist_train_labels.npy\") \n",
    "valX=np.load(\"mnist_validation_images.npy\") \n",
    "valY=np.load(\"mnist_validation_labels.npy\") \n",
    "testX=np.load(\"mnist_test_images.npy\") \n",
    "testY=np.load(\"mnist_test_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def J (W1, W2,b1,b2,faces, labels,alpha):  \n",
    "    z1=np.dot(faces,W1)+b1\n",
    "    h1=where(z1<0,0,z1)\n",
    "    z2=np.dot(h1,W2)+b2\n",
    "    f=exp(z2)\n",
    "    y_hat=f/f.sum(axis=1)[:,None]    \n",
    "    m=labels.shape[0]\n",
    "    Jsum=0\n",
    "    for i in range(0,m):\n",
    "        Jsum=Jsum+np.dot(log(y_hat[i,:]),labels[i,:].T)\n",
    "    Cost=-1/m*Jsum+alpha/2*(sum(diag(np.dot(W1.T,W1)))+sum(diag(np.dot(W2.T,W2))))\n",
    "    return Cost        \n",
    "\n",
    "def gradJ (W1, W2,b1,b2,faces, labels,alpha):\n",
    "    z1=np.dot(faces,W1)+b1\n",
    "    h1=where(z1<0,0,z1)\n",
    "    z2=np.dot(h1,W2)+b2\n",
    "    f=exp(z2)\n",
    "    y_hat=f/f.sum(axis=1)[:,None]  \n",
    "    \n",
    "    gt=np.dot((y_hat-labels),W2.T)*sign(z1)\n",
    "    m=labels.shape[0] \n",
    "    n=W1.shape[0]\n",
    "    p=W1.shape[1]\n",
    "    g=zeros([m,n*p])\n",
    "    for i in range(0,m):\n",
    "        for j in range(0,p):\n",
    "            g[i,n*j:n*(j+1)]=gt[i,j]*faces[i,:]\n",
    "    g2=np.sum(g,axis=0)   #Sum up all W1 gradient of all samples in given dataset\n",
    "    grad_W1=zeros([n,p])    \n",
    "    for j in range(0,p):\n",
    "        for i in range(0,n):\n",
    "            grad_W1[i,j]=g2[n*j+i]\n",
    "    grad_W1=grad_W1+alpha*W1       \n",
    "    grad_W2=np.dot(h1.T,(y_hat-labels))+alpha*W2 #Sum up all W2 gradient of all samples in given dataset\n",
    "    \n",
    "    grad_b1=np.sum(gt,axis=0)\n",
    "    grad_b2=np.sum(y_hat-labels,axis=0) #Sum up all b2 gradient of all samples in given dataset\n",
    "    #need to add grad_b1,grad_b2\n",
    "    return grad_W1,grad_W2 ,grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testAccuracy(valX,valY,W1,W2,b1,b2):\n",
    "    m=valY.shape[0]\n",
    "    b3=repeat(b1,m).reshape(m,b1.shape[1])\n",
    "    b4=repeat(b2,m).reshape(m,b2.shape[1])\n",
    "    z1=np.dot(valX,W1)+b3\n",
    "    h1=where(z1<0,0,z1)\n",
    "    z2=np.dot(h1,W2)+b4\n",
    "    f=exp(z2)\n",
    "    y_hat=f/f.sum(axis=1)[:,None] \n",
    "    alpha=0\n",
    "    J_test=J (W1, W2,b3,b4,valX,valY,alpha)\n",
    "    n=0\n",
    "    for i in range(0,m):\n",
    "        for j in range(0,10):\n",
    "            if (y_hat[i,j]==np.sort(y_hat[i,:])[-1]):\n",
    "                y_hat[i,j]=1\n",
    "            else:\n",
    "                y_hat[i,j]=0   \n",
    "\n",
    "        if (y_hat[i,:]==valY[i,:]).all():\n",
    "            n=n+1\n",
    "    a=round(n/m,4)\n",
    "    return J_test, a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SGD(faces, labels, num_units,alpha,epochs, mini_batch_size, eta, valX,valY):\n",
    "        W1=random.uniform(-1/sqrt(784),1/sqrt(784),size=(784,num_units)) \n",
    "        W2=random.uniform(-1/sqrt(num_units),1/sqrt(num_units),size=(num_units,10))\n",
    "        b1=0.01*ones([1, num_units])\n",
    "        b2=0.01*ones([1, 10])\n",
    "        for j in range(epochs):            \n",
    "            training_data=np.hstack([faces,labels])\n",
    "            random.shuffle(training_data)\n",
    "            m=labels.shape[0] \n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, m, mini_batch_size)]\n",
    "            for mini_batch in mini_batches[:-1]:\n",
    "                X= mini_batch[:,:784]\n",
    "                Y= mini_batch[:,-10:]\n",
    "                grad_W1,grad_W2,grad_b1,grad_b2= gradJ (W1,W2,b1,b2,X,Y,alpha)\n",
    "                W1=W1-eta/mini_batch_size*grad_W1\n",
    "                W2=W2-eta/mini_batch_size*grad_W2\n",
    "                b1=b1-eta/mini_batch_size*grad_b1\n",
    "                b2=b2-eta/mini_batch_size*grad_b2\n",
    "                J1 = J (W1, W2,b1,b2,X,Y,alpha)                    \n",
    "        J_test,a=testAccuracy(valX,valY,W1,W2,b1,b2)\n",
    "        return J_test,a\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findBestHyperparameters():\n",
    "    num_units_best=1\n",
    "    alpha_best=0.1\n",
    "    mini_batch_size_best=1\n",
    "    epochs_best=1\n",
    "    eta_best=0.1\n",
    "    a=0\n",
    "    for i in range(0,11):\n",
    "\n",
    "      \n",
    "        try:\n",
    "            num_units=int(input(\"num_units=\"))\n",
    "        except ValueError:\n",
    "            print(\"Not an integer value...please try again\")\n",
    "            num_units=int(input(\"num_units=\"))\n",
    "        eta=float(input(\"eta=\"))\n",
    "        try:\n",
    "            mini_batch_size=int(input(\"mini_batch_size=\"))\n",
    "        except ValueError:\n",
    "            print(\"Not an integer value...please try again\")\n",
    "            mini_batch_size=int(input(\"mini_batch_size=\"))\n",
    "        try:\n",
    "            epochs=int(input(\"epochs=\"))\n",
    "        except ValueError:\n",
    "            print(\"Not an integer value...please try again\")\n",
    "            epochs=int(input(\"epochs=\"))\n",
    "        alpha=float(input(\"alpha=\"))\n",
    "\n",
    "        Ji,ai=SGD(faces, labels, num_units,alpha,epochs, mini_batch_size, eta, valX,valY)\n",
    "        if ai>a:\n",
    "            a=ai\n",
    "            print(\"The best accuracy on the validation set until now: {};\".format(a))\n",
    "            num_units_best=num_units\n",
    "            alpha_best=alpha\n",
    "            mini_batch_size_best=mini_batch_size\n",
    "            epochs_best=epochs\n",
    "            eta_best=eta\n",
    "\n",
    "        else:\n",
    "            print(\"The best accuracy do not change this time\")\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "    print(\"Finally, The best accuracy on the validation set: {};\".format(a))\n",
    "    print(\"The best hyper parameters--\")\n",
    "    print(\"num_units_best: {};\".format(num_units_best))\n",
    "    print(\"alpha_best: {};\".format(alpha_best))\n",
    "    print(\"mini_batch_size_best: {};\".format(mini_batch_size_best))\n",
    "    print(\"epochs_best: {};\".format(epochs_best))\n",
    "    print(\"eta_best: {};\".format(eta_best))\n",
    "    return num_units_best, alpha_best,mini_batch_size_best, epochs_best,  eta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_units=30\n",
      "eta=0.5\n",
      "mini_batch_size=256\n",
      "epochs=5\n",
      "alpha=0.5\n",
      "The best accuracy on the validation set until now: 0.9386;\n",
      "-------------------------------------------------------------\n",
      "num_units=30\n",
      "eta=0.1\n",
      "mini_batch_size=128\n",
      "epochs=5\n",
      "alpha=0.1\n",
      "The best accuracy on the validation set until now: 0.9422;\n",
      "-------------------------------------------------------------\n",
      "num_units=30\n",
      "eta=0.1\n",
      "mini_batch_size=64\n",
      "epochs=10\n",
      "alpha=0.1\n",
      "The best accuracy on the validation set until now: 0.9562;\n",
      "-------------------------------------------------------------\n",
      "num_units=40\n",
      "eta=0.1\n",
      "mini_batch_size=256\n",
      "epochs=5\n",
      "alpha=0.1\n",
      "The best accuracy do not change this time\n",
      "-------------------------------------------------------------\n",
      "num_units=40\n",
      "eta=0.1\n",
      "mini_batch_size=128\n",
      "epochs=10\n",
      "alpha=0.1\n",
      "The best accuracy on the validation set until now: 0.9578;\n",
      "-------------------------------------------------------------\n",
      "num_units=40\n",
      "eta=0.01\n",
      "mini_batch_size=64\n",
      "epochs=10\n",
      "alpha=0.01\n",
      "The best accuracy do not change this time\n",
      "-------------------------------------------------------------\n",
      "num_units=40\n",
      "eta=0.01\n",
      "mini_batch_size=32\n",
      "epochs=15\n",
      "alpha=0.01\n",
      "The best accuracy do not change this time\n",
      "-------------------------------------------------------------\n",
      "num_units=50\n",
      "eta=0.5\n",
      "mini_batch_size=256\n",
      "epochs=5\n",
      "alpha=0.1\n",
      "The best accuracy on the validation set until now: 0.9636;\n",
      "-------------------------------------------------------------\n",
      "num_units=50\n",
      "eta=0.5\n",
      "mini_batch_size=128\n",
      "epochs=10\n",
      "alpha=0.1\n",
      "The best accuracy on the validation set until now: 0.9652;\n",
      "-------------------------------------------------------------\n",
      "num_units=50\n",
      "eta=0.1\n",
      "mini_batch_size=64\n",
      "epochs=10\n",
      "alpha=0.1\n",
      "The best accuracy on the validation set until now: 0.969;\n",
      "-------------------------------------------------------------\n",
      "num_units=50\n",
      "eta=0.01\n",
      "mini_batch_size=32\n",
      "epochs=20\n",
      "alpha=0.1\n",
      "The best accuracy do not change this time\n",
      "-------------------------------------------------------------\n",
      "Finally, The best accuracy on the validation set: 0.969;\n",
      "The best hyper parameters--\n",
      "num_units_best: 1;\n",
      "alpha_best: 0.1;\n",
      "epochs_best: 1;\n",
      "mini_batch_size_best: 1;\n",
      "eta_best: 0.1;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 0.1, 1, 1, 0.1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findBestHyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_units_best=50\n",
    "alpha_best=0.1\n",
    "mini_batch_size_best=64\n",
    "epochs_best=10\n",
    "eta_best=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "J_test,a_test=SGD(faces, labels, num_units_best,alpha_best,epochs_best, mini_batch_size_best, eta_best, testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-entropy cost on the testing set: 0.129\n",
      "The accuracy on the testing set:  96.39%\n"
     ]
    }
   ],
   "source": [
    "print(\"The cross-entropy cost on the testing set: {}\".format('%.3f' %J_test))\n",
    "print(\"The accuracy on the testing set: \"+\"{0: .2%}\".format(a_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
