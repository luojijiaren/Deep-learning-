{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess=tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "#load data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "\n",
    "W1=tf.Variable(tf.random_uniform([784,50],minval=-tf.sqrt(6.0)/(tf.sqrt(784.0+50.0)),maxval=tf.sqrt(6.0)/(tf.sqrt(784+50.0)),seed=1))\n",
    "W2=tf.Variable(tf.random_uniform([50,10],minval=-tf.sqrt(6.0)/(tf.sqrt(10+50.0)),maxval=tf.sqrt(6.0)/(tf.sqrt(10+50.0)),seed=3))\n",
    "b1=tf.Variable(tf.ones([50]))\n",
    "b2=tf.Variable(tf.ones([10]))\n",
    "\n",
    "#construct 3 layer network\n",
    "h1=tf.nn.relu(tf.matmul(x,W1)+b1)\n",
    "y_=tf.nn.softmax(tf.matmul(h1,W2)+b2)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_), reduction_indices=[1]))\n",
    "train_step=tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "\n",
    "#GSD with mini-batch\n",
    "for epoch in range(50):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/100)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = mnist.train.next_batch(100)\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c = sess.run([train_step,cross_entropy], feed_dict={x: batch_x, y: batch_y})\n",
    "        # Compute average loss\n",
    "        avg_cost += c / total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch >=30:\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "            \"{:.9f}\".format(avg_cost))\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "#get accuracy and cost of \n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "a_test=sess.run(accuracy,feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "c_test=sess.run(cross_entropy,feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "print(\"The accuracy on the testing set: \"+\"{0: .2%}\".format(a_test))\n",
    "print(\"The cross-entropy cost on the testing set: {}\".format('%.3f' %c_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess=tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "#load data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y=tf.placeholder(tf.float32,[None,10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial=tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "def bias_variable(shape):\n",
    "    initial=tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1=weight_variable([5,5,1,32])\n",
    "b_conv1=bias_variable([32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image=tf.reshape(x,[-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_conv1=tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)\n",
    "h_pool1=max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob=tf.placeholder(tf.float32)\n",
    "h_fc1_drop=tf.nn.dropout(h_fc1,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19980, training accuracy 1.0000\n",
      "step 19981, training accuracy 1.0000\n",
      "step 19982, training accuracy 1.0000\n",
      "step 19983, training accuracy 1.0000\n",
      "step 19984, training accuracy 1.0000\n",
      "step 19985, training accuracy 1.0000\n",
      "step 19986, training accuracy 1.0000\n",
      "step 19987, training accuracy 1.0000\n",
      "step 19988, training accuracy 1.0000\n",
      "step 19989, training accuracy 1.0000\n",
      "step 19990, training accuracy 1.0000\n",
      "step 19991, training accuracy 0.9800\n",
      "step 19992, training accuracy 1.0000\n",
      "step 19993, training accuracy 1.0000\n",
      "step 19994, training accuracy 1.0000\n",
      "step 19995, training accuracy 1.0000\n",
      "step 19996, training accuracy 1.0000\n",
      "step 19997, training accuracy 1.0000\n",
      "step 19998, training accuracy 1.0000\n",
      "step 19999, training accuracy 1.0000\n",
      "test accuracy 0.9916\n",
      "cross entropy 0.026\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i>=19980:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %.4f\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %.4f\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0}))\n",
    "print(\"cross entropy %.3f\"%cross_entropy.eval(feed_dict={\n",
    "    x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
