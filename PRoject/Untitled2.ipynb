{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data...\n",
      "Successfully downloaded data!\n",
      "Unzipping data...\n",
      "train already present - Skipping extraction of train.tar.gz.\n",
      "train\n",
      "test already present - Skipping extraction of test.tar.gz.\n",
      "test\n",
      "Extracting data for extra. This may take a while. Please wait.\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Compressed file ended before the end-of-stream marker was reached",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-27301aca54e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mtrain_folders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mtest_folders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mextra_folders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextra_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Successfully unzipped data!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-27301aca54e7>\u001b[0m in \u001b[0;36mmaybe_extract\u001b[0;34m(filename, force)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mtar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0mdata_folders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\fzhan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\tarfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[1;32m   1994\u001b[0m             \u001b[1;31m# Do not set_attrs directories, as we will do that further down\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m             self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\n\u001b[0;32m-> 1996\u001b[0;31m                          numeric_owner=numeric_owner)\n\u001b[0m\u001b[1;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1998\u001b[0m         \u001b[1;31m# Reverse sort directories.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\fzhan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\tarfile.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2036\u001b[0m             self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n\u001b[1;32m   2037\u001b[0m                                  \u001b[0mset_attrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset_attrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2038\u001b[0;31m                                  numeric_owner=numeric_owner)\n\u001b[0m\u001b[1;32m   2039\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrorlevel\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\fzhan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\tarfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misreg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2108\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2109\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2110\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\fzhan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\tarfile.py\u001b[0m in \u001b[0;36mmakefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2154\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2155\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2156\u001b[0;31m                 \u001b[0mcopyfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReadError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmakeunknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\fzhan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\tarfile.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mremainder\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremainder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mremainder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unexpected end of data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\fzhan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\fzhan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"B\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\fzhan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 raise EOFError(\"Compressed file ended before the \"\n\u001b[0m\u001b[1;32m    481\u001b[0m                                \"end-of-stream marker was reached\")\n\u001b[1;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Compressed file ended before the end-of-stream marker was reached"
     ]
    }
   ],
   "source": [
    "# Import Modules\n",
    "from __future__ import print_function\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import h5py\n",
    "from numpy import random\n",
    "\n",
    "# Download data\n",
    "print('Downloading data...')\n",
    "\n",
    "url = 'http://ufldl.stanford.edu/housenumbers/'\n",
    "\n",
    "def maybe_download(filename, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if force or not os.path.exists(filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "    print('Download Complete!')\n",
    "  statinfo = os.stat(filename)\n",
    "  return filename\n",
    "\n",
    "train_filename = maybe_download('train.tar.gz')\n",
    "test_filename = maybe_download('test.tar.gz')\n",
    "extra_filename = maybe_download('extra.tar.gz')\n",
    "\n",
    "print('Successfully downloaded data!')\n",
    "\n",
    "\n",
    "# Unzip Data\n",
    "print('Unzipping data...')\n",
    "np.random.seed(8)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  # Remove .tar.gz\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  \n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "  data_folders = root\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)\n",
    "extra_folders = maybe_extract(extra_filename)\n",
    "\n",
    "print('Successfully unzipped data!')\n",
    "\n",
    "# Create dictionary for bounding boxes\n",
    "print('Creating dictionary of bounding boxes...')\n",
    "class DigitStructFile:\n",
    "    def __init__(self, inf):\n",
    "        self.inf = h5py.File(inf, 'r')\n",
    "        self.digitStructName = self.inf['digitStruct']['name']\n",
    "        self.digitStructBbox = self.inf['digitStruct']['bbox']\n",
    "\n",
    "    def getName(self,n):\n",
    "        return ''.join([chr(c[0]) for c in self.inf[self.digitStructName[n][0]].value])\n",
    "\n",
    "    def bboxHelper(self,attr):\n",
    "        if (len(attr) > 1):\n",
    "            attr = [self.inf[attr.value[j].item()].value[0][0] for j in range(len(attr))]\n",
    "        else:\n",
    "            attr = [attr.value[0][0]]\n",
    "        return attr\n",
    "\n",
    "    def getBbox(self,n):\n",
    "        bbox = {}\n",
    "        bb = self.digitStructBbox[n].item()\n",
    "        bbox['height'] = self.bboxHelper(self.inf[bb][\"height\"])\n",
    "        bbox['label'] = self.bboxHelper(self.inf[bb][\"label\"])\n",
    "        bbox['left'] = self.bboxHelper(self.inf[bb][\"left\"])\n",
    "        bbox['top'] = self.bboxHelper(self.inf[bb][\"top\"])\n",
    "        bbox['width'] = self.bboxHelper(self.inf[bb][\"width\"])\n",
    "        return bbox\n",
    "    \n",
    "    def getDigitStructure(self,n):\n",
    "        s = self.getBbox(n)\n",
    "        s['name']=self.getName(n)\n",
    "        return s\n",
    "\n",
    "    def getAllDigitStructure(self):\n",
    "        return [self.getDigitStructure(i) for i in range(len(self.digitStructName))]\n",
    "\n",
    "    def getAllDigitStructure_ByDigit(self):\n",
    "        pictDat = self.getAllDigitStructure()\n",
    "        result = []\n",
    "        structCnt = 1\n",
    "        for i in range(len(pictDat)):\n",
    "            item = { 'filename' : pictDat[i][\"name\"] }\n",
    "            figures = []\n",
    "            for j in range(len(pictDat[i]['height'])):\n",
    "               figure = {}\n",
    "               figure['height'] = pictDat[i]['height'][j]\n",
    "               figure['label']  = pictDat[i]['label'][j]\n",
    "               figure['left']   = pictDat[i]['left'][j]\n",
    "               figure['top']    = pictDat[i]['top'][j]\n",
    "               figure['width']  = pictDat[i]['width'][j]\n",
    "               figures.append(figure)\n",
    "            structCnt = structCnt + 1\n",
    "            item['boxes'] = figures\n",
    "            result.append(item)\n",
    "        return result\n",
    "    \n",
    "print(\"Successfully created dictionary of bounding boxes!\")\n",
    "\n",
    "\n",
    "# Get Digit Structure\n",
    "print('Getting digit structure for training data...')\n",
    "digitFileTrain=DigitStructFile(os.path.join('train','digitStruct.mat'))\n",
    "train_data=digitFileTrain.getAllDigitStructure_ByDigit()\n",
    "print('Success!')\n",
    "\n",
    "print('Getting digit structure for test data...')\n",
    "digitFileTest=DigitStructFile(os.path.join('test','digitStruct.mat'))\n",
    "test_data=digitFileTest.getAllDigitStructure_ByDigit()\n",
    "print('Success!')\n",
    "\n",
    "\n",
    "print('Getting digit structure for extra data...')\n",
    "digitFileExtra=DigitStructFile(os.path.join('extra','digitStruct.mat'))\n",
    "extra_data=digitFileExtra.getAllDigitStructure_ByDigit()\n",
    "print('Success!')\n",
    "\n",
    "# Crop Training Images\n",
    "print('Cropping training images...')\n",
    "train_imsize = np.ndarray([len(train_data),2])\n",
    "for i in np.arange(len(train_data)):\n",
    "    filename = train_data[i]['filename']\n",
    "    fullname = os.path.join(train_folders, filename)\n",
    "    im = Image.open(fullname)\n",
    "    train_imsize[i, :] = im.size[:]\n",
    "\n",
    "print('Success!')\n",
    "\n",
    "# Crop Test Images\n",
    "print('Cropping test images...')\n",
    "test_imsize = np.ndarray([len(test_data),2])\n",
    "for i in np.arange(len(test_data)):\n",
    "    filename = test_data[i]['filename']\n",
    "    fullname = os.path.join(test_folders, filename)\n",
    "    im = Image.open(fullname)\n",
    "    test_imsize[i, :] = im.size[:]\n",
    "\n",
    "print('Success!')\n",
    "\n",
    "# Crop Extra Images\n",
    "print('Cropping extra images...')\n",
    "extra_imsize = np.ndarray([len(extra_data),2])\n",
    "for i in np.arange(len(extra_data)):\n",
    "    filename = extra_data[i]['filename']\n",
    "    fullname = os.path.join(extra_folders, filename)\n",
    "    im = Image.open(fullname)\n",
    "    extra_imsize[i, :] = im.size[:]\n",
    "\n",
    "print('Success!')\n",
    "\n",
    "# Use extra data\n",
    "def generate_dataset(data, folder):\n",
    "\n",
    "    dataset = np.ndarray([len(data),32,32,1], dtype='float32')\n",
    "    labels = np.ones([len(data),6], dtype=int) * 10\n",
    "    for i in np.arange(len(data)):\n",
    "        filename = data[i]['filename']\n",
    "        fullname = os.path.join(folder, filename)\n",
    "        im = Image.open(fullname)\n",
    "        boxes = data[i]['boxes']\n",
    "        num_digit = len(boxes)\n",
    "        labels[i,0] = num_digit\n",
    "        top = np.ndarray([num_digit], dtype='float32')\n",
    "        left = np.ndarray([num_digit], dtype='float32')\n",
    "        height = np.ndarray([num_digit], dtype='float32')\n",
    "        width = np.ndarray([num_digit], dtype='float32')\n",
    "        for j in np.arange(num_digit):\n",
    "            if j < 5: \n",
    "                labels[i,j+1] = boxes[j]['label']\n",
    "                if boxes[j]['label'] == 10: labels[i,j+1] = 0\n",
    "            else: print('#',i,'image has more than 5 digits.')\n",
    "            top[j] = boxes[j]['top']\n",
    "            left[j] = boxes[j]['left']\n",
    "            height[j] = boxes[j]['height']\n",
    "            width[j] = boxes[j]['width']\n",
    "        \n",
    "        im_top = np.amin(top)\n",
    "        im_left = np.amin(left)\n",
    "        im_height = np.amax(top) + height[np.argmax(top)] - im_top\n",
    "        im_width = np.amax(left) + width[np.argmax(left)] - im_left\n",
    "        \n",
    "        im_top = np.floor(im_top - 0.1 * im_height)\n",
    "        im_left = np.floor(im_left - 0.1 * im_width)\n",
    "        im_bottom = np.amin([np.ceil(im_top + 1.2 * im_height), im.size[1]])\n",
    "        im_right = np.amin([np.ceil(im_left + 1.2 * im_width), im.size[0]])\n",
    "\n",
    "        im = im.crop((im_left, im_top, im_right, im_bottom)).resize([32,32], Image.ANTIALIAS)\n",
    "        im = np.dot(np.array(im, dtype='float32'), [[0.2989],[0.5870],[0.1140]])\n",
    "        mean = np.mean(im, dtype='float32')\n",
    "        std = np.std(im, dtype='float32', ddof=1)\n",
    "        if std < 1e-4: std = 1.\n",
    "        im = (im - mean) / std\n",
    "        dataset[i,:,:,:] = im[:,:,:]\n",
    "\n",
    "    return dataset, labels\n",
    "\n",
    "print('Generating training dataset and labels...')\n",
    "train_dataset, train_labels = generate_dataset(train_data, train_folders)\n",
    "print('Success! \\n Training set: {} \\n Training labels: {}'.format(train_dataset.shape, train_labels.shape))\n",
    "\n",
    "\n",
    "print('Generating testing dataset and labels...')\n",
    "test_dataset, test_labels = generate_dataset(test_data, test_folders)\n",
    "print('Success! \\n Testing set: {} \\n Testing labels: {}'.format(test_dataset.shape, test_labels.shape))\n",
    "\n",
    "print('Generating extra dataset and labels...')\n",
    "extra_dataset, extra_labels = generate_dataset(extra_data, extra_folders)\n",
    "print('Success! \\n Testing set: {} \\n Testing labels: {}'.format(extra_dataset.shape, extra_labels.shape))\n",
    "\n",
    "\n",
    "# Clean up data by deleting digits more than 5 (very few)\n",
    "print('Cleaning up training data...')\n",
    "train_dataset = np.delete(train_dataset, 29929, axis=0)\n",
    "train_labels = np.delete(train_labels, 29929, axis=0)\n",
    "print('Success!')\n",
    "\n",
    "# Expand Training Data\n",
    "print('Expanding training data randomly...')\n",
    "\n",
    "random.seed(8)\n",
    "\n",
    "n_labels = 10\n",
    "valid_index = []\n",
    "valid_index2 = []\n",
    "train_index = []\n",
    "train_index2 = []\n",
    "for i in np.arange(n_labels):\n",
    "    valid_index.extend(np.where(train_labels[:,1] == (i))[0][:400].tolist())\n",
    "    train_index.extend(np.where(train_labels[:,1] == (i))[0][400:].tolist())\n",
    "    valid_index2.extend(np.where(extra_labels[:,1] == (i))[0][:200].tolist())\n",
    "    train_index2.extend(np.where(extra_labels[:,1] == (i))[0][200:].tolist())\n",
    "\n",
    "random.shuffle(valid_index)\n",
    "random.shuffle(train_index)\n",
    "random.shuffle(valid_index2)\n",
    "random.shuffle(train_index2)\n",
    "\n",
    "valid_dataset = np.concatenate((extra_dataset[valid_index2,:,:,:], train_dataset[valid_index,:,:,:]), axis=0)\n",
    "valid_labels = np.concatenate((extra_labels[valid_index2,:], train_labels[valid_index,:]), axis=0)\n",
    "train_dataset_new = np.concatenate((extra_dataset[train_index2,:,:,:], train_dataset[train_index,:,:,:]), axis=0)\n",
    "train_labels_new = np.concatenate((extra_labels[train_index2,:], train_labels[train_index,:]), axis=0)\n",
    "\n",
    "print('Success! \\n Training set: {} \\n Training labels: {}'.format(train_dataset_new.shape, train_labels_new.shape))\n",
    "print('Success! \\n Validation set: {} \\n Validation labels: {}'.format(valid_dataset.shape, valid_labels.shape))\n",
    "print('Success! \\n Testing set: {} \\n Testing labels: {}'.format(test_dataset.shape, test_labels.shape))\n",
    "\n",
    "\n",
    "# Create Pickling File\n",
    "print('Pickling data...')\n",
    "pickle_file = 'SVHN.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'train_dataset': train_dataset_new,\n",
    "        'train_labels': train_labels_new,\n",
    "        'valid_dataset': valid_dataset,\n",
    "        'valid_labels': valid_labels,\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "        }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to {}: {}'.format(pickle_file, e))\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Success!')\n",
    "print('Compressed pickle size: {}'.format(statinfo.st_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Loading pickled data...')\n",
    "\n",
    "pickle_file = 'SVHN.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    X_train = save['train_dataset']\n",
    "    y_train = save['train_labels']\n",
    "    X_val = save['valid_dataset']\n",
    "    y_val = save['valid_labels']\n",
    "    X_test = save['test_dataset']\n",
    "    y_test = save['test_labels']\n",
    "    del save  \n",
    "    print('Training data shape:', X_train.shape)\n",
    "    print('Training label shape:',y_train.shape)\n",
    "    print('Validation data shape:', X_val.shape)\n",
    "    print('Validation label shape:', y_val.shape)\n",
    "    print('Test data shape:', X_test.shape)\n",
    "    print('Test label shape:', y_test.shape)\n",
    "\n",
    "print('Data successfully loaded!')\n",
    "\n",
    "\n",
    "\n",
    "print('Defining accuracy function...')\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels)\n",
    "            / predictions.shape[1] / predictions.shape[0])\n",
    "print('Accuracy function defined!')\n",
    "\n",
    "# CNN Model\n",
    "print('Loading data and building computation graph...')\n",
    "\n",
    "'''Basic information'''\n",
    "# We processed image size to be 32\n",
    "image_size = 32\n",
    "# Number of channels: 1 because greyscale\n",
    "num_channels = 1\n",
    "# Mini-batch size\n",
    "batch_size = 16\n",
    "# Number of output labels\n",
    "num_labels = 11\n",
    "\n",
    "'''Filters'''\n",
    "# depth: number of filters (output channels) - should be increasing\n",
    "# num_channels: number of input channels set at 1 previously\n",
    "patch_size = 5\n",
    "depth_1 = 16\n",
    "depth_2 = depth_1 * 2\n",
    "depth_3 = depth_2 * 3\n",
    "\n",
    "# Number of hidden nodes in fully connected layer 1\n",
    "num_hidden = 64\n",
    "shape = [batch_size, image_size, image_size, num_channels]\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    '''Input Data'''\n",
    "    # X_train: (223965, 32, 32, 1)\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "\n",
    "    # y_train: (223965, 7)\n",
    "    tf_train_labels = tf.placeholder(\n",
    "        tf.int32, shape=(batch_size, 6))\n",
    "\n",
    "    # X_val: (11788, 32, 32, 1)\n",
    "    tf_valid_dataset = tf.constant(X_val)\n",
    "\n",
    "    # X_test: (13067, 32, 32, 1)\n",
    "    tf_test_dataset = tf.constant(X_test)\n",
    "\n",
    "    '''Variables'''\n",
    "\n",
    "    # Create Variables Function\n",
    "    def init_weights(shape, name):\n",
    "        return tf.Variable(\n",
    "            tf.random_normal(shape=shape, stddev=0.01),\n",
    "            name=name)\n",
    "\n",
    "    def init_biases(shape, name):\n",
    "        return tf.Variable(\n",
    "            tf.constant(1.0, shape=shape),\n",
    "            name=name\n",
    "        )\n",
    "\n",
    "    # Create Function for Image Size: Pooling\n",
    "    # 3 Convolutions\n",
    "    # 2 Max Pooling\n",
    "    def output_size_pool(input_size, conv_filter_size, pool_filter_size,\n",
    "                         padding, conv_stride, pool_stride):\n",
    "        if padding == 'same':\n",
    "            padding = -1.00\n",
    "        elif padding == 'valid':\n",
    "            padding = 0.00\n",
    "        else:\n",
    "            return None\n",
    "        # After convolution 1\n",
    "        output_1 = (\n",
    "            ((input_size - conv_filter_size - 2 * padding) / conv_stride) + 1.00)\n",
    "        # After pool 1\n",
    "        output_2 = (\n",
    "            ((output_1 - pool_filter_size - 2 * padding) / pool_stride) + 1.00)\n",
    "        # After convolution 2\n",
    "        output_3 = (\n",
    "            ((output_2 - conv_filter_size - 2 * padding) / conv_stride) + 1.00)\n",
    "        # After pool 2\n",
    "        output_4 = (\n",
    "            ((output_3 - pool_filter_size - 2 * padding) / pool_stride) + 1.00)\n",
    "        # After convolution 2\n",
    "        output_5 = (\n",
    "            ((output_4 - conv_filter_size - 2 * padding) / conv_stride) + 1.00)\n",
    "        # After pool 2\n",
    "        # output_6 = (\n",
    "        #     ((output_5 - pool_filter_size - 2 * padding) / pool_stride) + 1.00)\n",
    "        return int(output_5)\n",
    "\n",
    "    # Convolution 1\n",
    "    # Input channels: num_channels = 1\n",
    "    # Output channels: depth = depth_1\n",
    "    w_c1 = init_weights([patch_size, patch_size, num_channels, depth_1], 'w_c1')\n",
    "    b_c1 = init_biases([depth_1], 'b_c1')\n",
    "\n",
    "    # Convolution 2\n",
    "    # Input channels: num_channels = depth_1\n",
    "    # Output channels: depth = depth_2\n",
    "    w_c2 = init_weights([patch_size, patch_size, depth_1, depth_2], 'w_c2')\n",
    "    b_c2 = init_biases([depth_2], 'b_c2')\n",
    "\n",
    "    # Convolution 3\n",
    "    # Input channels: num_channels = depth_2\n",
    "    # Output channels: depth = depth_3\n",
    "    w_c3 = init_weights([patch_size, patch_size, depth_2, depth_3], 'w_c3')\n",
    "    b_c3 = init_biases([depth_3], 'b_c3')\n",
    "\n",
    "    # Fully Connect Layer 1\n",
    "    final_image_size = output_size_pool(input_size=image_size,\n",
    "                                        conv_filter_size=5, pool_filter_size=2,\n",
    "                                        padding='valid', conv_stride=1,\n",
    "                                        pool_stride=2)\n",
    "    print('Final image size after convolutions {}'.format(final_image_size))\n",
    "    w_fc1 = init_weights([final_image_size*final_image_size*depth_3, num_hidden], 'w_fc1')\n",
    "    b_fc1 = init_biases([num_hidden], 'b_fc1')\n",
    "\n",
    "    # Softmax 1\n",
    "    w_s1 = init_weights([num_hidden, num_labels], 'w_s1')\n",
    "    b_s1 = init_biases([num_labels], 'b_s1')\n",
    "\n",
    "    # Softmax 2\n",
    "    w_s2 = init_weights([num_hidden, num_labels], 'w_s2')\n",
    "    b_s2 = init_biases([num_labels], 'b_s2')\n",
    "\n",
    "    # Softmax 3\n",
    "    w_s3 = init_weights([num_hidden, num_labels], 'w_s3')\n",
    "    b_s3 = init_biases([num_labels], 'b_s3')\n",
    "\n",
    "    # Softmax 4\n",
    "    w_s4 = init_weights([num_hidden, num_labels], 'w_s4')\n",
    "    b_s4 = init_biases([num_labels], 'b_s4')\n",
    "\n",
    "    # Softmax 5\n",
    "    w_s5 = init_weights([num_hidden, num_labels], 'w_s5')\n",
    "    b_s5 = init_biases([num_labels], 'b_s5')\n",
    "\n",
    "    def model(data, keep_prob, shape):\n",
    "        with tf.name_scope(\"conv_layer_1\"):\n",
    "            conv_1 = tf.nn.conv2d(\n",
    "                data, w_c1, strides=[1, 1, 1, 1], padding='VALID')\n",
    "            hidden_conv_1 = tf.nn.relu(conv_1 + b_c1)\n",
    "            pool_1 = tf.nn.max_pool(\n",
    "                hidden_conv_1, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "        with tf.name_scope(\"conv_layer_2\"):\n",
    "            conv_2 = tf.nn.conv2d(\n",
    "                pool_1, w_c2, strides=[1, 1, 1, 1], padding='VALID')\n",
    "            hidden_conv_2 = tf.nn.relu(conv_2 + b_c2)\n",
    "            pool_2 = tf.nn.max_pool(\n",
    "                hidden_conv_2, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "        with tf.name_scope(\"conv_layer_3\"):\n",
    "            conv_3 = tf.nn.conv2d(\n",
    "                pool_2, w_c3, strides=[1, 1, 1, 1], padding='VALID')\n",
    "            hidden_conv_3 = tf.nn.relu(conv_3 + b_c3)\n",
    "        with tf.name_scope(\"fc_layer_1\"):\n",
    "            hidden_drop = tf.nn.dropout(hidden_conv_3, keep_prob)\n",
    "            shape = hidden_drop.get_shape().as_list()\n",
    "            reshape = tf.reshape(\n",
    "                hidden_drop, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden_fc = tf.nn.relu(\n",
    "                tf.matmul(reshape, w_fc1) + b_fc1)\n",
    "        with tf.name_scope(\"softmax_1\"):\n",
    "            logits_1 = tf.matmul(hidden_fc, w_s1) + b_s1\n",
    "        with tf.name_scope(\"softmax_2\"):\n",
    "            logits_2 = tf.matmul(hidden_fc, w_s2) + b_s2\n",
    "        with tf.name_scope(\"softmax_3\"):\n",
    "            logits_3 = tf.matmul(hidden_fc, w_s3) + b_s3\n",
    "        with tf.name_scope(\"softmax_4\"):\n",
    "            logits_4 = tf.matmul(hidden_fc, w_s4) + b_s4\n",
    "        with tf.name_scope(\"softmax_5\"):\n",
    "            logits_5 = tf.matmul(hidden_fc, w_s5) + b_s5\n",
    "        return [logits_1, logits_2, logits_3, logits_4, logits_5]\n",
    "\n",
    "    '''Training Computation'''\n",
    "    [logits_1, logits_2, logits_3, logits_4, logits_5] = model(\n",
    "        tf_train_dataset, 0.5, shape)\n",
    "\n",
    "    '''Loss Function'''\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                   logits_1, tf_train_labels[:, 1])) + \\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                   logits_2, tf_train_labels[:, 2])) + \\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                   logits_3, tf_train_labels[:, 3])) + \\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                   logits_4, tf_train_labels[:, 4])) + \\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                   logits_5, tf_train_labels[:, 5]))\n",
    "        # Add scalar summary for cost\n",
    "        tf.scalar_summary(\"loss\", loss)\n",
    "\n",
    "    '''Optimizer'''\n",
    "    # Decaying learning rate\n",
    "    # count the number of steps taken\n",
    "    global_step = tf.Variable(0)\n",
    "    start_learning_rate = 0.05\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        start_learning_rate, global_step, 100000, 0.96, staircase=True)\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "        loss, global_step=global_step)\n",
    "\n",
    "    '''Predictions'''\n",
    "    def softmax_combine(dataset, shape):\n",
    "        train_prediction = tf.pack([\n",
    "            tf.nn.softmax(model(dataset, 1.0, shape)[0]),\n",
    "            tf.nn.softmax(model(dataset, 1.0, shape)[1]),\n",
    "            tf.nn.softmax(model(dataset, 1.0, shape)[2]),\n",
    "            tf.nn.softmax(model(dataset, 1.0, shape)[3]),\n",
    "            tf.nn.softmax(model(dataset, 1.0, shape)[4])])\n",
    "        return train_prediction\n",
    "\n",
    "    train_prediction = softmax_combine(tf_train_dataset, shape)\n",
    "    valid_prediction = softmax_combine(tf_valid_dataset, shape)\n",
    "    test_prediction = softmax_combine(tf_test_dataset, shape)\n",
    "\n",
    "    '''Save Model (will be initiated later)'''\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    '''Histogram for Weights'''\n",
    "    # Add histogram summaries for weights\n",
    "    tf.histogram_summary(\"w_c1_summ\", w_c1)\n",
    "    tf.histogram_summary(\"b_c1_summ\", b_c1)\n",
    "\n",
    "    tf.histogram_summary(\"w_c2_summ\", w_c2)\n",
    "    tf.histogram_summary(\"b_c2_summ\", b_c2)\n",
    "\n",
    "    tf.histogram_summary(\"w_c3_summ\", w_c3)\n",
    "    tf.histogram_summary(\"b_c3_summ\", b_c3)\n",
    "\n",
    "    tf.histogram_summary(\"w_fc1_summ\", w_fc1)\n",
    "    tf.histogram_summary(\"b_fc1_summ\", b_fc1)\n",
    "\n",
    "    tf.histogram_summary(\"w_s1_summ\", w_s1)\n",
    "    tf.histogram_summary(\"b_s1_summ\", b_s1)\n",
    "\n",
    "    tf.histogram_summary(\"w_s2_summ\", w_s2)\n",
    "    tf.histogram_summary(\"b_s2_summ\", b_s2)\n",
    "\n",
    "    tf.histogram_summary(\"w_s3_summ\", w_s3)\n",
    "    tf.histogram_summary(\"b_s3_summ\", b_s3)\n",
    "\n",
    "    tf.histogram_summary(\"w_s4_summ\", w_s4)\n",
    "    tf.histogram_summary(\"b_s4_summ\", b_s4)\n",
    "\n",
    "    tf.histogram_summary(\"w_s5_summ\", w_s5)\n",
    "    tf.histogram_summary(\"b_s5_summ\", b_s5)\n",
    "\n",
    "print('Data loaded and computation graph built!')\n",
    "\n",
    "num_steps = 60000\n",
    "\n",
    "print('Running computation and iteration...')\n",
    "print('If you are unable to save the summary, please change the path to where you want it to write.')\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    writer = tf.train.SummaryWriter(\"/log_trial_1\", session.graph)  # for 0.8\n",
    "    merged = tf.merge_all_summaries()\n",
    "\n",
    "    '''If you want to restore model'''\n",
    "    # saver.restore(session, \"model_trial_1.ckpt\")\n",
    "    # print(\"Model restored!\")\n",
    "\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "        batch_data = X_train[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data,\n",
    "                     tf_train_labels: batch_labels}\n",
    "        _, l, predictions, summary = session.run([optimizer, loss, train_prediction, merged],\n",
    "                                        feed_dict=feed_dict)\n",
    "        writer.add_summary(summary)\n",
    "        if (step % 500 == 0):\n",
    "            print(('Minibatch loss at step {}: {}').format(step, l))\n",
    "            print(\n",
    "            ('Minibatch accuracy: {}%'.format(accuracy(predictions, batch_labels[:,1:6]))))\n",
    "            print(\n",
    "            ('Validation accuracy: {}%'.format(accuracy(valid_prediction.eval(),\n",
    "                                                     y_val[:,1:6]))))\n",
    "    print(\n",
    "    ('Test accuracy: {}%'.format(accuracy(test_prediction.eval(), y_test[:,1:6]))))\n",
    "\n",
    "    save_path = saver.save(session, \"model_trial_1.ckpt\")\n",
    "    print('Model saved in file: {}'.format(save_path))\n",
    "\n",
    "\n",
    "print('Successfully completed computation and iterations!')\n",
    "\n",
    "print('To view Tensorboard\\'s visualizations, please run \\\n",
    "\\'tensorboard --logdir=log_trial_1\\' in your terminal')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
